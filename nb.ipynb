{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "folder_path = \"/home/cerrion/DATATHON/data/hackathon_data\"\n",
    "files_in_folder = os.listdir(folder_path)\n",
    "\n",
    "len(files_in_folder)\n",
    "def load_documents(json_file):\n",
    "    \"\"\"Loads the JSON file.\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "      try:\n",
    "          data = json.load(f)\n",
    "          return data\n",
    "      except json.JSONDecodeError:\n",
    "          print(f\"Error reading {json_file}, it may not be a valid JSON file.\")\n",
    "    return []\n",
    "\n",
    "for filename in files_in_folder:\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        doc = load_documents(file_path)\n",
    "        break\n",
    "print(doc.keys())\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_items_per_doc = []\n",
    "char_counts = []\n",
    "\n",
    "long_texts = []   # Pages longer than 1M characters\n",
    "short_texts = []  # Pages with 0 characters\n",
    "\n",
    "LONG_TEXT_THRESHOLD = 1_000_000  # 1 million characters\n",
    "SHORT_TEXT_THRESHOLD = 0         # Zero-length pages\n",
    "MAX_SAVED = 30                   # Cap long/short saves for inspection\n",
    "\n",
    "for filename in tqdm(files_in_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        doc = load_documents(file_path)\n",
    "\n",
    "        text_by_page = doc.get('text_by_page_url', {})\n",
    "        num_items = len(text_by_page)\n",
    "        num_items_per_doc.append(num_items)\n",
    "\n",
    "        for page_url, text in text_by_page.items():\n",
    "            length = len(text)\n",
    "            char_counts.append(length)\n",
    "\n",
    "            # Save long texts\n",
    "            if length > LONG_TEXT_THRESHOLD and len(long_texts) < MAX_SAVED:\n",
    "                long_texts.append({\n",
    "                    \"source_file\": filename,\n",
    "                    \"page_url\": page_url,\n",
    "                    \"char_length\": length,\n",
    "                    \"text\": text\n",
    "                })\n",
    "\n",
    "            # Save short texts (length == 0)\n",
    "            if length == SHORT_TEXT_THRESHOLD and len(short_texts) < MAX_SAVED:\n",
    "                short_texts.append({\n",
    "                    \"source_file\": filename,\n",
    "                    \"page_url\": page_url,\n",
    "                    \"char_length\": length,\n",
    "                    \"text\": text\n",
    "                })\n",
    "\n",
    "# Output paths (one dir up from input folder)\n",
    "base_output_path = os.path.abspath(os.path.join(folder_path, \"..\"))\n",
    "long_output_path = os.path.join(base_output_path, \"long_texts_over_1M.json\")\n",
    "short_output_path = os.path.join(base_output_path, \"short_texts_empty.json\")\n",
    "\n",
    "# Save long texts\n",
    "with open(long_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(long_texts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save short texts\n",
    "with open(short_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(short_texts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Final output\n",
    "print(\"\\n📊 Summary Statistics:\")\n",
    "print(f\"Number of items (pages) per document: {num_items_per_doc}\")\n",
    "print(f\"Total number of text blocks processed: {len(char_counts)}\")\n",
    "print(f\"Example character counts per text block: {char_counts[:10]}\")\n",
    "print(f\"\\n📝 Saved {len(long_texts)} long text blocks to: {long_output_path}\")\n",
    "print(f\"📝 Saved {len(short_texts)} short (empty) text blocks to: {short_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "lengths_array = np.array(char_counts)\n",
    "log_lengths = np.log10(lengths_array + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(log_lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"log10(Text length in characters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Text Lengths (Character Count, log-scale)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "items_array = np.array(num_items_per_doc)\n",
    "log_items = np.log10(items_array + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(log_items, bins=50, color='salmon', edgecolor='black')\n",
    "plt.xlabel(\"log10(Number of pages per document)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Pages per Document (log-scale)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
